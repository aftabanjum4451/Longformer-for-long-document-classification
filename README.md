# Longformer for longer document classification
The project is about how to employ a twelve-head transformer with local and global attention mechanisms, and the model is referred to as Long-former (long-former for sequence classification with twelve head and global attention mechanism ).
The information I utilised in this project is private. As a result, the data cannot be shared for public use.
